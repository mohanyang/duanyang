\ifx \allfiles \undefined
\documentclass{article}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}

\begin{document}
\title{Method}
\maketitle \else \fi

\section{Experiments}\label{sec:experiment}
In this section, we test our different models on real dataset extracted from twitter. We name snowball algorithm as SA, random surfing algorithm as RSA, naive bayes algorithm as NBA and the combination of random surfing algorithm and naive bayes algorithm as CM. In the experiments, we use precision at the position $K$ to evaluate the accuracy of our ranking result. The precision at the position $K$ measures how many related user we ranked in top $K$ positions, thus it penalizes the model which performs poorly in top positions. Here we didn't use recall as one of our metrics since that we can hardly obtain the exactly number of users belong to our target category.

\subsection{Experimental Setup}
\textbf{User data:} The users used to train and evaluate our algorithm are collected from twitter during October, 2011. As described in section TODO, we collected TODO users from our seeds users. For each user, we also collected his at most 20 tweets in October, his profile and his followings. Thus, in total we have TODO tweets with more than TODO different words. Since user word pairs are very sparse and some word appeared in only a few users are not generalizable, we eliminate the words appeared less than 100 times and there is TODO word left.

\textbf{Label data:} During our experiments, we mainly focus on predicting users from four categories, they are UCLA, Stanford, MIT and USC. The user belong to these four universities may highly connected with each other. Later in our experiments part, we will prove that some of our algorithm will still have good performance with dataset that is not highly connected. For category of each university, we use $80\%$ users with target keyword in their short bio as training data and erase the target keyword appeared in other $20\%$ users. In the evaluation process, we will also label top TODO results by human in order to obtain a more accuracy evaluation result.

\subsection{Ranking Performance}
In this part of the experiments, we use our label data and sort the users with respect to the estimated relevance given by our algorithm. We show the detailed precision curve (at different position) for different categories in Figure TODO.

From Figure TODO we can see that the TODO and TODO performs the best at all most every positions. This fact demonstrates that TODO. Note that much of the improvement in the precision curve comes in the area near the position TODO to TODO. This is the area that we most care about, since that users in top positions have some features that is extremely easy to discover and users in these middle positions reflect the effectiveness of our algorithm more clearly.

We also show some really related users and unrelated users ranked by our algorithm in top positions in Table TODO. This table reflects our program's strengths and weaknesses. The SA TODO. The RSA TODO. The NBA TODO. The CM TODO.

Moreover, we show some of the features importance of NBA in table \ref{tab:keyword}. We ranked features according to their weight $\log p(c|w)$ and pick up the top $10$ words for different categories. From the table, we could learn some different attributes related to particular category of users. For example, UCLA students like ``dailybruin'' news, call themselves as ``bruins'' and lived near ``westwood''; USC students like news from ``usc annenberge'' and the idol statue in their university is ``Trojan''; MIT students live in ``Cambridge'' and there is famous ``media lab'' in their computer science department; Stanford students are glad to talking about their athletic team using nick name ``cardinal''.

\begin{table*}[t]
\centering
\begin{tabular}{|l|l|}
\hline
UCLA & dailybruin, bruin, bruins, westwod, neuheisel, alumna, wooden, undergraduate, midterm, royce \\
\hline
USC & ascj, uscedu, annenberg, uscpsycho, uscannenberg, ausc, beattheirish, trojan, trojans, atrojan \\
\hline
MIT & sloan, medialab, cambridge, joi, kayak, bostonupdate, alums, mechanical, techreview, edu \\
\hline
Stanford & stanfordfball, gostanford, astanford, gsb, cardinal, cantor, tristanwalker, auditorium, alums, freshmen \\
\hline
\end{tabular}
\caption{The keywords in short bio and tweets of users from different universities.}\label{tab:keyword}
\end{table*}

\subsection{Ranking with Loss of Information}
In this part of the experiments, we evaluate our different models with the loss of information. The loss of information means that during the training process, we erase some users profile in label data and treat these users as normal users without our target profile keywords. We test our program and plot the results for different amount of training data in Figure TODO.

The results show that with the loss of information, the performance of SA algorithm and RSA algorithm falls down dramatically since that the prediction based in these algorithms mainly based on graph structure and link relationship between label users. The NBA TODO. As we expect that CM algorithm TODO.

In our original dataset, the training users are likely to connected with each other since they are in same university. However, for other categories such as father, gamer or phd, users belong to such categories maybe not likely to connected with others in same category. When we erase the profile keyword in label data, the connection in label data become smaller and the structure may like other categories. The performance of our algorithms also demonstrate that our algorithms could be applied to different categories.

\ifx \allfiles \undefined
\end{document}
\fi

%I think we need rewrite this part. Thanks.

%Our crawler will begin with Twitter users from four universities including UCLA, USC, Stanford and MIT, and perform a two-step random walk starting from these users to obtain a larger set of different background users. Every user's profile, followers and followings information will be retrieved. The crawler will also retrieve these users' tweets posted after a specific time.
%We will analysis the number of followers, followings, tweets and retweets for each user, and quantify a user's influence on these metrics using a PageRank like mechanism.
%We will analysis the tweet keywords for each user, and provide a metric to evaluate the tweets similarity between two users. We will also compare the interest difference between users in different universities.
%We will quantify two users' closeness using their tweet and reply (regarding retweet as a kind of reply) behavior data. Our model is similar to the model used in predicting a chess game result based on two players' past campaign results \cite{elo1986rating}.
%We will propose an algorithm for extracting a user's hidden profile. Each edge will be weighted by the closeness and tweets similarity of the corresponding two users. The algorithm performs a random walk on the weighted graph to collect all possible hidden profile for a specific user.

\subsection{Data Collection}
We selected 20 seed users from each of the four universities, including UCLA, USC, Stanford and MIT. User profile (id, location, screen name, number of followings and followers, and a short biography) of these seed users was crawled using the Twitter API. Starting from these seed users, a two-step random walk is performed to retrieve their followers and their followers' followers. If a new user from these four universities is discovered during the procedure, this user is marked as a seed user and another two-step random walk starting from this user is performed. The crawler has collected more than 540,000 users and 780,000,000 following relations. It has also collected the most recent 20 tweets for each user.

\subsection{Data Analysis}
Keywords are extracted from user profiles. Keyword frequencies are calculated, and some keywords are manually labeled into three categories, namely location (CA, Los Angeles, NY, etc.), occupation (CEO, writer, blogger, student, etc.) and affiliation (UCLA, USC, Stanford, MIT, Google, etc.). Following is some statistics about the users in different universities (a subset of affiliation).

\subsubsection{Who follows whom on twitter?}
Table 1 shows the average numbers of followings in different universities with respect to users in different universities. 'Other' represents that the user does not mention any of these four universities in his biography. For example, the third row and second column means 38.17 followings of a UCLA user is in other universities on average. It is shown that the number of followers in the same university is ten times larger than that in different universities. However, this only counts for 5\% among all a user's followers. A possible reason might be that many users do not write their universities in their biographies.

%Maybe we don't need this

%\subsubsection{Who replies/retweets to whom on twitter?}
%More than 75,000 replies/retweets are collected. 86\% of them are between a user in these four universities and a user in affiliation "Other". For the rest replies/retweets, 98\% of them are between users in the same universities.
