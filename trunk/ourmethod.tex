\ifx \allfiles \undefined
\documentclass{article}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}

\begin{document}
\title{Method}

\newcommand{\following}{\ensuremath{following}}
\newcommand{\follower}{\ensuremath{follower}}

\maketitle \else \fi

\section{Our Approach}\label{sec:method}

\subsection{Snowball Model}
%In considering the operational models for the extraction of a profile keyword of a user through a social network $G$, we will first treat this problem as an information influence spread processing.

Given a category $\mathcal{C}$ and a keyword $z_{\mathcal{C}}$ which best describes $\mathcal{C}$, each user $u \in V$ has an active score $s_u$ representing the likelihood that $u$ belongs to category $\mathcal{C}$. If $s_u \geq 1$, user $u$ is called active, indicating $u$ belongs to $\mathcal{C}$; otherwise, $u$ is called inactive. The snowball algorithm tries to estimate $s_u$ using the influence propagation in the network. 

%Given keyword $z_{\mathcal{C}}$, users are partitioned into sets $\mathcal{A}$ and $\mathcal{B}$. The users in $\mathcal{A}$ are active, and users in set $\mathcal{B}$ are inactive. We focus on settings guided by previous work about the spread of influence in which each user's state to become active increases monotonically as more of its friends become actives. The snowball algorithm is an iterative algorithm. Once a user become active, it remains active in the future iterations. For an inactive user $u$, more and more followings of $u$ become active after many iterations, and $u$ might become active if enough
%as time passed, more and more of users followed by $u$ become active; at some point, this cause $u$ to become active, and $u$ influenced his followers.

As shown in Algorithm~\ref{alg:snowball}, the snowball algorithm is an iterative algorithm. Initially, the active score of users in $\mathcal{A}$ is set to 1, while the active score of users in $\mathcal{B}$ is set to 0. So only users in $\mathcal{A}$ are active at the beginning. During each iteration, the algorithm scans through each inactive user $u$ and calculate its new active score based on the active score of $u$'s followings:
\begin{equation}\label{eq:snowball}
s_u = \sum_{v \in \following(u)} \frac{s_v}{|\follower(v)|}
\end{equation}
If the new active score $s_u$ is no less than 1, then user $u$ becomes active. Although user $u$ is inactive at the beginning, he might become active as many of his following becomes active during the iteration, and he will be able to contribute to his followers once he becomes active. The algorithm iterates until no more users could become active. Users in $\mathcal{B}$ are ranked based on the time at which they become active.
%In order to obtain the ranking result for $z_{\mathcal{C}}$, here we rank the inactive users in $D$ according to their time point to become an active user.

%------------------------------------------------
\begin{algorithm}[htbp]
\caption{\textsc{Snowball}}
\textbf{Input: }{Category $\mathcal{C}$ and keyword $z_{\mathcal{C}}$}\\
\textbf{Output: }{An array $rank$ containing the users in $\mathcal{B}$ ranked on the probability of belonging to $\mathcal{C}$}
\begin{algorithmic}[1]
\STATE $s_u \leftarrow 1$ {\bf for} $u \in \mathcal{A}$, $s_u \leftarrow 0$ {\bf for} $u \in \mathcal{B}$
\WHILE{$rank$ changed during last iteration}
\FOR{$u \in V$ s.t. $s_u < 1$}
    \STATE update $s_u$ according to Eqn.~(\ref{eq:snowball})
    \STATE {\bf if} $s_u \geq 1$, {\bf then} add $u$ to $rank$
\ENDFOR
\ENDWHILE
\STATE {\bf for} $u \in V$ s.t. $s_u < 1$ {\bf do} add $u$ to $rank$
\RETURN $rank$
\end{algorithmic}
\label{alg:snowball}
\end{algorithm}
%------------------------------------------------

\subsection{Random Work Model}
Considering the graph structure generated by twitter social network, it is similar to web graph structure where users are pages, and there are some outgoing links from $u$ to $v$ since that $u$ is following $v$, and also incoming links from $v$ to $u$ since that $u$ is followed by $v$.

In order to predict the relationship between keywords or phrase and users, a second general approach is to predict the relationship importance between users in $D$ and users in $L$. For user $u$, an easy way to rank the importance of other users to him is performing the random walk from $u$. After the random walk starting from user $u$ go through his friends links, we obtain the rank of users in which top ranked users might be most important guy that $u$ follows. By the random walk starting from user $u$ go through his follower links, we obtain the rank of users in which top ranked users might be most important guy that followed $u$. For a specified or phrase $z$, we add undirected links from $z$ to users have this keyword. And then, after the random walk from user $u$, we get how important that $z$ is related to $u$ in both directions.

However, we don't have computation power to perform this random walk since that there are too many users in social network. We modify this model in reverse direction as the Trust-rank algorithm. For keyword or phrase $z_i$, we use $a_j$ represent the importance that user $u_j$ is related to $z_i$ in friends direction and $s_j$ represent the importance that $u_j$ is related to $z_i$ in followers direction. Additionally, we use vector $A=(a_1, a_2, ..., a_n)$ and $S=(s_1, s_2, ..., s_n)$.

For a set of users $L$ contains target profile $z_i$, we assign their initial weight $\frac{1}{|L|}$ in vector $A$ and $S$. A user $u_j$ received his weight $a_j$ from each of his friends with probability equal to $\frac{1}{\following(u_j)}$. And $u_j$ received his weight $s_j$ from each of his followers with probability $\frac{1}{\follower(u_j)}$. Thus, the iterative process of random walk can be defined as follows:
$$A = (1 - d) M_a A + d B$$
$$S = (1 - d) M_s S + d B$$
Here $B$ is a vector represent the trust value of each user. $B_i = \frac{1}{|L|}$ for each user $u_i$ in set $L$. $M_a$ and $M_s$ is $n \times n$ transition matrix as described above. $\alpha, \beta$ are normalize parameters and $d$ controls the percentage of trust score in our algorithm.

Finally, we rank the user according to the value $r_i=a_i \times b_i$ since $r_i$ represent the importance of profile $z_i$ from both friends and followers directions.

\subsection{Naive Bayes Model}
Both two models described above works well in some situations. However, we ignore the tweets information and other profile information in our dataset. With the information extracted from our dataset, another general approach to our problem would be to view it as a classification task. We take users that already have some profile keyword as positive training examples, and treat other users as negative training examples. Then we could learn a classifier that predicts how likely that a user is going to have a profile keyword.

There are several machine learning algorithms that address this problem. Here we use a Naive Bayesian model which is one of the most effective and efficient classification algorithm. In this algorithm, the learner attempts to construct a classifier for a set of training example with class labels. Assume that $X_1, X_2, \cdots X_n$ are training vectors $X_i=(x_1, x_2, \cdots, x_m)$ and each training vector has a class label $c$. The Bayes estimate $p(x_j|c)$ from the training examples to maximize the probability:
$$\prod_i p(c)\prod_jp(x_j|c)$$

In our paper, we apply this Naive Bayes method to learn the probability that user $u_j$ has the profile $z_i$. And return the sorted list according the probability.
However, the dataset for this classification problem has several disadvantage. The first is that the class is imbalance, for each profile $z_i$, there will be a very small fraction of the users in the network and learning is particularly hard in domains with high class imbalance. The second challenge is that there are huge amount of features. There are lots different words appeared in users tweets and also typos and connected words. Therefore, we modify the Naive Bayes algorithm so that for each profile keyword $z_i$, the learner could give user relevant score according to:
$$r = \frac{1}{n_i}\sum_{i=1}^{n_i} \log p(c|w_i)$$

\subsection{Model Combination}
The Random Walk model works well for profile which users with this profile like connected with each other. For example, users in same companies or universities would like to follow each other. The Naive Bayes model works well for profile which users with this profile like to talk about same topics. For example, users like comic probably didn't know each other and follow each, but they tweet same words for same topic.

There is a general approach to combine two different models together and generate better result for different kind of profile keyword.
We perform an iteration algorithm that combines Naive Bayes model and Random Walk model, we choose a parameter $d$. During each iteration, we put top $d$ result generated by Naive Bayes model into Random Walk model as users in $L$ and put top $d$ result generated by Random Walk model into Naive Bayesian model as users in $L$. We perform this iteration several rounds until top results generated by Random Walk model and Naive Bayes model becomes the same.

We arrive at Algorithm 4 that iteratively computes the eigenvector $A$, $B$ as well as the relevant score $r$.
%I will write sudo code here

\ifx \allfiles \undefined
\end{document}
\fi
