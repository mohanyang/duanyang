\ifx \allfiles \undefined
\documentclass{article}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}

\begin{document}
\title{Method}

\newcommand{\following}{\ensuremath{following}}
\newcommand{\follower}{\ensuremath{follower}}

\maketitle \else \fi

\section{Our Approach}\label{sec:method}

\subsection{Snowball Model}
%In considering the operational models for the extraction of a profile keyword of a user through a social network $G$, we will first treat this problem as an information influence spread processing.

Given a category $\mathcal{C}$ and a keyword $z_{\mathcal{C}}$ which best describes $\mathcal{C}$, each user $u \in V$ has an active score $s_u$ representing the likelihood that $u$ belongs to category $\mathcal{C}$. If $s_u \geq 1$, user $u$ is called active, indicating $u$ belongs to $\mathcal{C}$; otherwise, $u$ is called inactive. The snowball algorithm tries to estimate $s_u$ using the influence propagation in the network.

%Given keyword $z_{\mathcal{C}}$, users are partitioned into sets $\mathcal{A}$ and $\mathcal{B}$. The users in $\mathcal{A}$ are active, and users in set $\mathcal{B}$ are inactive. We focus on settings guided by previous work about the spread of influence in which each user's state to become active increases monotonically as more of its friends become actives. The snowball algorithm is an iterative algorithm. Once a user become active, it remains active in the future iterations. For an inactive user $u$, more and more followings of $u$ become active after many iterations, and $u$ might become active if enough
%as time passed, more and more of users followed by $u$ become active; at some point, this cause $u$ to become active, and $u$ influenced his followers.

As shown in Algorithm~\ref{alg:snowball}, the snowball algorithm is an iterative algorithm. Initially, the active score of users in $\mathcal{A}$ is set to 1, while the active score of users in $\mathcal{B}$ is set to 0. So only users in $\mathcal{A}$ are active at the beginning. During each iteration, the algorithm scans through each inactive user $u$ and calculate its new active score based on the active score of $u$'s followings:
\begin{equation}\label{eq:snowball}
s_u = \sum_{v \in \following(u)} \frac{s_v}{|\follower(v)|}
\end{equation}
If the new active score $s_u$ is no less than 1, then user $u$ becomes active. Although user $u$ is inactive at the beginning, he might become active as many of his following becomes active during the iteration, and he will be able to contribute to his followers once he becomes active. The algorithm iterates until no more users could become active. Users in $\mathcal{B}$ are ranked based on the time at which they become active.
%In order to obtain the ranking result for $z_{\mathcal{C}}$, here we rank the inactive users in $D$ according to their time point to become an active user.

%------------------------------------------------
\begin{algorithm}[htbp]
\caption{\textsc{Snowball}}
\textbf{Input: }{Category $\mathcal{C}$ and keyword $z_{\mathcal{C}}$}\\
\textbf{Output: }{An array $rank$ containing the users in $\mathcal{B}$ ranked on the probability of belonging to $\mathcal{C}$}
\begin{algorithmic}[1]
\STATE $s_u \leftarrow 1$ {\bf for} $u \in \mathcal{A}$, $s_u \leftarrow 0$ {\bf for} $u \in \mathcal{B}$
\WHILE{$rank$ changed during last iteration}
\FOR{$u \in V$ s.t. $s_u < 1$}
    \STATE update $s_u$ according to Eqn.~(\ref{eq:snowball})
    \STATE {\bf if} $s_u \geq 1$, {\bf then} add $u$ to $rank$
\ENDFOR
\ENDWHILE
\STATE {\bf for} $u \in V$ s.t. $s_u < 1$ {\bf do} add $u$ to $rank$
\RETURN $rank$
\end{algorithmic}
\label{alg:snowball}
\end{algorithm}
%------------------------------------------------

\subsection{Random Walk Model}

The underlying graph structure of twitter social network is similar to that of the Web graph, where a user in twitter corresponds to a page in the Web, and a following relation between two users corresponds to a directed hyperlink between two pages. Existing methods like PageRank\cite{page1999pagerank}, TrustRank\cite{gyngyi2004combating} and random walks with restarts\cite{tong2006fast} are popular random walk based methods for ranking nodes in the Web graph. Our second approach uses a variant of TrustRank to calculate the relevance score $s_u$ for each user $u \in \mathcal{B}$. The idea is user $v$ positively contributes to $u$'s relevance score if $v$ is $u$'s following, and user $w$ also positively contributes to $u$'s relevance score if $w$ is $u$'s follower.

%Considering the graph structure generated by twitter social network, it is similar to web graph structure where users are pages, and there are some outgoing links from $u$ to $v$ since that $u$ is following $v$, and also incoming links from $v$ to $u$ since that $u$ is followed by $v$.

%In order to predict the relationship between keywords or phrase and users, a second general approach is to predict the relationship importance between users in $D$ and users in $L$. For user $u$, an easy way to rank the importance of other users to him is performing the random walk from $u$. After the random walk starting from user $u$ go through his friends links, we obtain the rank of users in which top ranked users might be most important guy that $u$ follows. By the random walk starting from user $u$ go through his follower links, we obtain the rank of users in which top ranked users might be most important guy that followed $u$. For a specified or phrase $z$, we add undirected links from $z$ to users have this keyword. And then, after the random walk from user $u$, we get how important that $z$ is related to $u$ in both directions.

Define two vectors, $P = (p_1, \cdots, p_n)$ and $Q = (q_1, \cdots, q_n)$, where $p_u$ represents the relevance score of user $u$ to category $\mathcal{C}$ with respect to the following direction, and $q_u$ represents the relevance score with respect to the follower direction.
%However, we don't have computation power to perform this random walk since that there are too many users in social network. We modify this model in reverse direction as the Trust-rank algorithm. For keyword or phrase $z_i$, we use $a_j$ represent the importance that user $u_j$ is related to $z_i$ in friends direction and $s_j$ represent the importance that $u_j$ is related to $z_i$ in followers direction. Additionally, we use vector $P = (p_1, p_2, \cdots, p_n)$ and $Q = (q_1, q_2, \cdots, q_n)$.
In the following direction, $u$ receives weight from each of his followings with probability $(1 - d) / |\following(u)|$. On the other hand, $u$ receives weight from each of his followers with probability $(1 - d) / |\follower(u)|$ in the follower direction. The random walk is formulated as:
\begin{eqnarray}\label{eq:randomwalk}
P & = & (1 - d) M_P P + d T \nonumber \\
Q & = & (1 - d) M_Q Q + d T
\end{eqnarray}
Vector $T$ represents the trust value of each user, where $T_u = \frac{1}{|\mathcal{A}|}$ for $u \in \mathcal{A}$ and $T_u = 0$ for $u \in \mathcal{B}$. $M_P$ and $M_Q$ are $n \times n$ transition matrices described above. $d$ is the damping factor.

The initial value of $p_u$ and $q_u$ is ${1 \over |\mathcal{A}|}$ for $u \in \mathcal{A}$, otherwise it is 0. Each iteration updates the value of $P$ and $Q$ based on Eqn.~(\ref{eq:randomwalk}), and normalizes vector $P$ and $Q$ into unit vectors. Finally, the users are ranked according to the relevance score $s_u = p_u q_u$ after convergence, which is a combination of relevance to category $\mathcal{C}$ from both following and follower directions.

\subsection{Naive Bayes Model}

The above two models works well in many situations. However, the tweets information are ignored in these two approaches. Our third approach builds a classifier using the tweets information, and predicts how likely a user in $\mathcal{B}$ belongs to category $\mathcal{C}$. The users in $\mathcal{A}$ and $\mathcal{B}$ are positive training examples and negative training examples, respectively.

%Both two models described above works well in some situations. However, we ignore the tweets information and other profile information in our dataset. With the information extracted from our dataset, another general approach to our problem would be to view it as a classification task. We take users that already have some profile keyword as positive training examples, and treat other users as negative training examples. Then we could learn a classifier that predicts how likely that a user is going to have a profile keyword.

There are many machine learning algorithms which address this classification problem. We use the Naive Bayes classifier which is one of the most effective and efficient classification algorithm. In this algorithm, the learner attempts to construct a classifier for a set of labeled training examples.
%There are several machine learning algorithms that address this problem. Here we use a Naive Bayes model which is one of the most effective and efficient classification algorithm. In this algorithm, the learner attempts to construct a classifier for a set of training example with class labels. 
Assume that $X_1, X_2, \cdots X_n$ are training vectors $X_i=(x_1, x_2, \cdots, x_m)$ and each training vector has a class label $c$. The Bayes estimate $p(x_j|c)$ from the training examples to maximize the probability:
$$\prod_i p(c)\prod_jp(x_j|c)$$

%In our paper, we apply this Naive Bayes method to learn the probability that user $u_j$ has the profile $z_i$. And return the sorted list according the probability.

However, the dataset for this classification problem has several disadvantages. Firstly, the class is unbalanced. The fraction of positive training samples is too small, which makes the learning particularly difficult. Secondly, the number of features is very large, which is due to the diversity of words people used in their tweets and frequent appearance of typos and hyphen marks. We modify the Naive Bayes algorithm to calculate the relevant score according to the following equation:
%The first is that the class is imbalance, for each profile $z_i$, there will be a very small fraction of the users in the network and learning is particularly hard in domains with high class imbalance. The second challenge is that there are huge amount of features. There are lots different words appeared in users tweets and also typos and connected words. Therefore, we modify the Naive Bayes algorithm so that for each profile keyword $z_i$, the learner could give user relevant score according to:
\begin{equation}
r = \frac{1}{n_i}\sum_{i=1}^{n_i} \log p(c|w_i)
\end{equation}

\subsection{Model Combination}

The random walk model works well for profile which users with this profile like connected with each other. For example, users in same companies or universities would like to follow each other. The Naive Bayes model works well for profile which users with this profile like to talk about same topics. For example, users like comic probably didn't know each other and follow each, but they tweet same words for same topic.

%There is a general approach to combine two different models together and generate better result for different kind of profile keyword.
A more powerful approach combines the random walk model and Naive Bayes model together, and iteratively reinforce the result of one model by the the result of another model. Let $l$ are $k$ be positive parameters, and $k > l$. During each iteration, top-$l$ users generated by the Naive Bayes model considered as trusted users in the new iteration of random walk model, and the top-$l$ users generated by the new random walk iteration is considered as positive training examples in the Naive Bayes model. The iteration is executed until the top-$k$ users generated by random walk model and Naive Bayes model are the same.

We arrive at Algorithm 4 that iteratively computes the vector $P$, $Q$ as well as the relevant score $s_u$.
%I will write sudo code here

\ifx \allfiles \undefined
\end{document}
\fi
